# Memory Augmented LSTMs
## Title - “Memory on Demand: Can Augmented LSTMs Rival Transformers in Few-Shot Sentiment Analysis?”

### Summary
State-of-the-art sentiment analysis models like e.g., Transformer-based models including BERT, RoBERTa, and DeBERTa have shown good few-shot learning performance but at costly computation and with heavy pretraining. In contrast, LSTM-based models are less computationally demanding but suffer from catastrophic forgetting and hence have their knowledge retention capability for various tasks restricted. This work investigates if MA-LSTMs can serve as an efficient, but lightweight, alternative to Transformers in the context of few-shot sentiment analysis. By embedding a differentiable memory module (i.e., Neural Dictionary, Key-Value Memory), an attempt is made to extend the ability of LSTMs in storing and recalling earlier information to mitigate forgetting at low loss in efficiency. The research aims to establish whether MA-LSTMs can achieve competitive accuracy, improved memory retention, and greater computational efficiency compared to Transformer-based models when applied to few-shot learning. If successful, the method could provide a less resource-hungry alternative to current Transformer-bloated solutions.

### Results
Memory on Demand is a few shot sentiment analysis system that combines a lightweight bi directional LSTM (two layers, 256 units per direction) with an external differentiable key value memory bank. Trained meta episodically on five-shot support and query splits, the Memory LSTM achieves 93.75 percent in-domain five-shot accuracy, matching a DistilBERT-based ProtoNet at 91.3 percent while using only 1.7 million parameters versus 22.7 million and delivering roughly 0.002 seconds per sample inference. It also generalizes strongly out of distribution, scoring 76.3 percent accuracy on held-out IMDB, and exhibits minimal catastrophic forgetting a 1.75 percentage-point gain after sequential fine-tuning compared to a 16 percentage-point drop for a vanilla LSTM. Paired t tests (p < 0.0001) and comparisons to ProtoNet (60 percent) and NNShot (51 percent) confirm its statistical robustness and efficiency. The full system : transformer, vanilla LSTM, and Memory LSTM baselines is publicly deployed at 
https://huggingface.co/spaces/AravindMohan/myFewshotSentimentAnalyser. 
